# LLM Chat

Simple llm chat client for ollama.

# Developing

- ollama server should be running:
```
./ollama serve
```
- start the dev server:
```
npm run dev
```
